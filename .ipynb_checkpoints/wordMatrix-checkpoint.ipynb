{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word matrix\n",
    "Goal: Convert unstructured text into a document matrix  \n",
    "Julian Pulido\n",
    "STAT 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necesary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       FDC IS DEDICATED TO MEETING THE NEEDS OF A DIV...\n",
       "1       Offer unique opportunity for the high-school s...\n",
       "2       TO PROVIDE TRANSITIONAL HOUSING, PERMANENT HOU...\n",
       "3       NATIONAL HONOR SOCIETY FOR COLLEGE STUDENTS IN...\n",
       "4       PROVIDE PERMANENT SUPPORTIVE HOUSING FOR INDIV...\n",
       "                              ...                        \n",
       "6449    THE CENTRAL IOWA TRAUMA RECOVERY CENTER (CITRC...\n",
       "6450    Accompany Capital creates a pathway to prosper...\n",
       "6451    TO ENSURE ALL MICHIGAN CHILDREN AND YOUTH WITH...\n",
       "6452    Advocacy Unlimited is a CT based peer-led orga...\n",
       "6453    \"GUIDED BY THE TEACHINGS OF JESUS CHRIST, SAIN...\n",
       "Name: mission, Length: 6454, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Julian\\\\Downloads\\\\descriptions.csv\")\n",
    "data[\"mission\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example of how a word was stemmed.  \n",
    "I used the same function from last homework to stem and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence example:  This is my homework for big data and an this is an example sentence. Today is a hot day.\n",
      "Stemmed and stop words removed: ['homework', 'big', 'data', 'exampl', 'sentenc', 'today', 'hot', 'day']\n"
     ]
    }
   ],
   "source": [
    "#create stemmer object\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "#create Count Vectorizer veector \n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#function to stem words and remove stop words\n",
    "def stemmed_words1(doc):\n",
    "\t# Better to use builtin:\n",
    "  #stopwords = {\"it\", \"to\", \"the\"}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docwords = [stemmer.stem(w) for w in analyzer(doc)]\n",
    "    return [x for x in docwords if x.lower() not in stop_words]\n",
    "\n",
    "#create new count vectorizer object with the analyzer being the funciton above\n",
    "cv1 = CountVectorizer(analyzer=stemmed_words1)\n",
    "\n",
    "\n",
    "example = \"This is my homework for big data and an this is an example sentence. Today is a hot day.\"\n",
    "print(\"Sentence example: \" , example)\n",
    "\n",
    "print(\"Stemmed and stop words removed:\" , stemmed_words1(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the dimensions of your matrix?\n",
    "6454 rows, 8257 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6454, 8257)\n",
      "(6454, 8257)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#CV\n",
    "#Fit it to our data\n",
    "#CV is count vectorizer\n",
    "cv1.fit(data[\"mission\"])\n",
    "\n",
    "\n",
    "# What does each column in the matrix (feature) represent?\n",
    "words =cv1.get_feature_names_out()\n",
    "\n",
    "# The actual matrix\n",
    "m = cv1.transform(data[\"mission\"])\n",
    "\n",
    "print(m.shape)\n",
    "\n",
    "#Tfidf \n",
    "#for our tfidf, re-weight the count features into floating point\n",
    "V = TfidfVectorizer(analyzer=stemmed_words1)\n",
    "V.fit(data[\"mission\"])\n",
    "\n",
    "#transform into tdidf sparse matrix. this scales so that its in euclidean norm 1\n",
    "tfidf = V.transform(data[\"mission\"])\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many bytes of memory are used to represent the matrix in sparse form?\n",
    "48 bytes of memory are used to represent the matrix in sparse form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our matrix in sparse form in bytes: 48\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of our matrix in sparse form in bytes:\" , sys.getsizeof(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many bytes of memory would used to represent the matrix in dense form? (Don't actually convert it to dense- it will be too large!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes in dense form: 426325424\n",
      "MB of matrix in dense form: 426.325424\n"
     ]
    }
   ],
   "source": [
    "denseFormSize = 6454 * 8257* 8 \n",
    "print(\"Bytes in dense form:\" , denseFormSize)\n",
    "print(\"MB of matrix in dense form:\", denseFormSize/10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if you try to transform a document that contains some words that were not in the original corpus? Does it work?\n",
    "It still works, but it does not count the word that was not in the original corpus. My sparse matrix only contains values in two locations, but does account for the special word 'brusque' that does not pop up in the orginal corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example document: ['brusque is the word of the day']\n",
      "Is the word 'brusque' in the original corpus? False\n",
      "Word at 2196 index:  day \n",
      "Word at 8152 index: word\n",
      "  (0, 2196)\t1\n",
      "  (0, 8152)\t1\n"
     ]
    }
   ],
   "source": [
    "#example doc\n",
    "print(\"Example document:\" , doc)\n",
    "doc = [\"brusque is the word of the day\"]\n",
    "print(\"Is the word 'brusque' in the original corpus?\" , \"brusque\" in cv1.get_feature_names_out())\n",
    "\n",
    "test = cv1.transform(doc)\n",
    "print(\"Word at 2196 index: \", words[2196] , \"\\nWord at 8152 index:\" , words[8152])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the top 10 most frequent words? Are they meaningful, or should they be removed?\n",
    "The 10 most popular words are meaningful and should be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of wordCounts  (1, 8257)\n",
      "Shape of orignal count vectorized matrix: (6454, 8257)\n",
      "Most popular words:  ['organ' 'student' 'famili' 'support' 'educ' 'communiti' 'servic' 'provid'\n",
      " 'health' 'mental']\n",
      "Least popular words:  ['jeopard' 'educationon' 'educationw' 'educatyion' 'educt' 'edutain'\n",
      " 'edututorva' 'persian' 'efec' 'perpetr']\n"
     ]
    }
   ],
   "source": [
    "#get the sum of each unique word into a matrix. axis =1 is rows\n",
    "#use axis =0 for the columns. sum down the column for each word\n",
    "wordCounts = m.sum(axis=0)\n",
    "\n",
    "#wordCounts is a list of lists. check with shape\n",
    "print(\"Shape of wordCounts \", wordCounts.shape)\n",
    "\n",
    "#fix using np.ravel to return a 1d contiguous flattned array\n",
    "wordCounts = np.ravel(wordCounts)\n",
    "\n",
    "#use argsort to get sort the word counts, but we only save their indexes\n",
    "rank = np.argsort(wordCounts)\n",
    "print(\"Shape of orignal count vectorized matrix:\", m.shape)\n",
    "#print(m.sum(axis=0)[:,4637])\n",
    "rank.shape\n",
    "\n",
    "#our words was a list of all the unique words. use rank (a list of indexes) \n",
    "#to create a new list of popular words thats sorted based on counts\n",
    "#so last word had the most times appeard\n",
    "popularWords = words[rank]\n",
    "\n",
    "\n",
    "print(\"Most popular words: \", popularWords[-10:])\n",
    "print(\"Least popular words: \", popularWords[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(rank)\n",
    "#rank.shape\n",
    "#rank[-10:]\n",
    "#m.shape\n",
    "#print(m.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with the min_df, max_df arguments to CountVectorizerLinks to an external site.. Which values did you choose, and why?\n",
    "\n",
    "I created a function and edited the `min_df` to be 5. This means that the CountVectorizer will ignore words that appeared less than 5 documents. I chose this value because if a word appears in less than 5 documents out of 6454, then that likely means that the word is mispelled or a very unique word that other non profits don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of count vectorizer matrix (6454, 2302)\n",
      "Shape of wordCounts  (1, 2302)\n",
      "(2302,)\n",
      "Most popular words:  ['ccsd' 'correct' 'assault' 'coup' 'anguish' 'addit' 'conceiv' 'ciri'\n",
      " 'basketbal' 'butler']\n",
      "Least popular words:  ['compatriot' 'appropri' 'acommun' 'control' 'cielo' 'airport' '2014'\n",
      " 'circl' '72' 'cerebr']\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer\n",
    "#using min_df =0.01 ignores all words that appeared less than 0.01 of documents\n",
    "\n",
    "def removeSparseWords(lowerBound = 0):\n",
    "  \"\"\"\n",
    "  Function to remove sparse words. Creates a count vectorizer and returns a list of the top 10 least and most popular words\n",
    "\n",
    "  \"\"\"\n",
    "  cv2 = CountVectorizer(analyzer=stemmed_words1, min_df= lowerBound)\n",
    "\n",
    "  #fit the new model on the mission column\n",
    "  cv2.fit(data[\"mission\"])\n",
    "\n",
    "  # The actual matrix\n",
    "  m2 = cv2.transform(data[\"mission\"])\n",
    "\n",
    "  print(\"Shape of count vectorizer matrix\", m2.shape)\n",
    "\n",
    "  wordCounts = m2.sum(axis=0)\n",
    "\n",
    "  #wordCounts is a list of lists. check with shape\n",
    "  print(\"Shape of wordCounts \", wordCounts.shape)\n",
    "\n",
    "  #fix using np.ravel to return a 1d contiguous flattned array\n",
    "  wordCounts = np.ravel(wordCounts)\n",
    "\n",
    "  print(wordCounts.shape)\n",
    "  \n",
    "  #use argsort to get sort the word counts, but we only save their indexes\n",
    "  rank = np.argsort(wordCounts)\n",
    "\n",
    "\n",
    "  #our words was a list of all the unique words. use rank (a list of indexes) \n",
    "  #to create a new list of popular words thats sorted based on counts\n",
    "  #so last word had the most times appeard\n",
    "  popularWords = words[rank]\n",
    "\n",
    "  print(\"Most popular words: \", popularWords[-10:])\n",
    "  print(\"Least popular words: \", popularWords[:10])\n",
    "\n",
    "removeSparseWords(lowerBound= 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Python to pick a random nonprofit. Verify that the bag of words approach does the correct operation on this particular description, both in the counts and the TF-IDF.\n",
    "\n",
    "From a random non profit's mission statement, I am going to calculate the sum of unique words and compare that to the sum of unique words from my matrix for the same mission statement. If this is the same then that verifies that my bag of words approach was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mission statement DISBURSE PAYMENTS TO OR ON BEHALF OF PARTICIPANTS RELATED TO MEDICAL, DENTAL, MENTAL HEALTH AND PRESCRIPTION DRUGS.\n",
      "Stemmed mission statement ['disburs', 'payment', 'behalf', 'particip', 'relat', 'medic', 'dental', 'mental', 'health', 'prescript', 'drug']\n",
      "Sum of words after stemmed:  11\n",
      "Same sum compared to my matrix? True\n",
      "  (0, 6258)\t0.2611507965641592\n",
      "  (0, 5881)\t0.4662346282216208\n",
      "  (0, 5608)\t0.4308232712431329\n",
      "  (0, 5563)\t0.28834383153918935\n",
      "  (0, 4766)\t0.0989896286383229\n",
      "  (0, 4739)\t0.25946449952181866\n",
      "  (0, 3533)\t0.11045303485675782\n",
      "  (0, 2512)\t0.30732909856923896\n",
      "  (0, 2282)\t0.34177120729796506\n",
      "  (0, 1052)\t0.3809137667572531\n",
      "(1, 8257)\n",
      "2.945473763209459\n",
      "Same sum of for TFIDF for the same mission statement?\n",
      "3.093906843500359\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "#get random index from 1 to number of rows of data\n",
    "randomIndex = random.randint(1, data.shape[0])\n",
    "\n",
    "#get mission statement at random Index\n",
    "missionStatement = data.iloc[randomIndex,2]\n",
    "print(\"mission statement\" , missionStatement)\n",
    "\n",
    "#stem the words using our function\n",
    "stemmedStatement = stemmed_words1(missionStatement)\n",
    "print(\"Stemmed mission statement\" , stemmedStatement)\n",
    "\n",
    "#compare the number of words and see if that matches to what we have in our matrix\n",
    "print(\"Sum of words after stemmed: \"  , len(stemmedStatement))\n",
    "print(\"Same sum compared to my matrix?\" , m[randomIndex].sum() == len(stemmedStatement))\n",
    "\n",
    "#our stemmedStament is a list of all words. I want all the words in a single element of a list since it was part of the whole sentence\n",
    "combinedStatment  = ' '.join(stemmedStatement)\n",
    "\n",
    "#clear our list\n",
    "stemmedStatement = stemmedStatement.clear\n",
    "\n",
    "#set the first element to be the sentence\n",
    "stemmedStatement = [combinedStatment]\n",
    "\n",
    "\n",
    "#transform our stemmed Staement into a sparse matrix of tfidf values. only has one row of our stemmed statement\n",
    "tfidfList = V.transform(stemmedStatement)\n",
    "print(tfidfList)\n",
    "print(tfidfList.shape)\n",
    "print(tfidfList[0].sum())\n",
    "print(\"Same sum of for TFIDF for the same mission statement?\" )\n",
    "print(tfidf[randomIndex].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
