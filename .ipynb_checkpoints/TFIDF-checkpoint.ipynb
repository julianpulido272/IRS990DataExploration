{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31562bf9-3ee5-4de8-bbb7-8682830b42ad",
   "metadata": {},
   "source": [
    "Julian Pulido\n",
    "STAT 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab129db-ef06-4a92-9719-9faddd8ddf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d79c2-6a13-4c82-83e5-fd7734ca3dec",
   "metadata": {},
   "source": [
    "**#1 include only the bare minimum of code necessary to create the TFIDF matrix for the sample corpus. Add comments describing what each step in the processing does.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad388d1-0ca1-4b26-a492-f7c78d94a765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus is a collection of documents\n",
    "corpus = [\n",
    "        \"It's going to be raining tomorrow\",\n",
    "        \"Rain, rain, go away\",\n",
    "        \"It's rainy, so let's go to the library\",\n",
    "]\n",
    "\n",
    "# Create the object representing the model\n",
    "cv0 = CountVectorizer()\n",
    "\n",
    "# Fit it to our data\n",
    "cv0.fit(corpus)\n",
    "\n",
    "# What does each column in the matrix (feature) represent?\n",
    "cv0.get_feature_names_out()\n",
    "\n",
    "# The actual matrix\n",
    "m = cv0.transform(corpus)\n",
    "\n",
    "# DANGER! Convert to dense.\n",
    "m.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebb74bb-c696-4a9a-b40e-bf889c149fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing first sentence that had different stem fucntions applied\n",
      "['it', 'go', 'to', 'be', 'rain', 'tomorrow']\n",
      "['go', 'rain', 'tomorrow']\n",
      "\n",
      "\n",
      "TFIDF Matrix after stemming and removing stop words\n",
      "['away' 'go' 'let' 'librari' 'rain' 'raini' 'tomorrow']\n",
      "[[0 1 0 0 1 0 1]\n",
      " [1 1 0 0 2 0 0]\n",
      " [0 1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Following example at\n",
    "# https://stackoverflow.com/a/36191362\n",
    "stemmer = EnglishStemmer()\n",
    "#stemmer = EnglishStemmer(ignore_stopwords = True)\n",
    "\n",
    "#create Count Vectorizer veector \n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "# for each letter in the document, stem the word\n",
    "def stemmed_words(doc):\n",
    "    return [stemmer.stem(w) for w in analyzer(doc)]\n",
    "\n",
    "#stem the words of the first sentence\n",
    "stemmed_words(corpus[0])\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#function to stem words and remove stop words\n",
    "def stemmed_words1(doc):\n",
    "\t# Better to use builtin:\n",
    "  #stopwords = {\"it\", \"to\", \"the\"}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docwords = [stemmer.stem(w) for w in analyzer(doc)]\n",
    "    return [x for x in docwords if x.lower() not in stop_words]\n",
    "\n",
    "#create new model object with the analyzer being the funciton above\n",
    "cv1 = CountVectorizer(analyzer=stemmed_words1)\n",
    "\n",
    "#fit the model to the corpus\n",
    "cv1.fit(corpus)\n",
    "\n",
    "#compare the first sentence that were stemmed differently\n",
    "print(\"Comparing first sentence that had different stem fucntions applied\")\n",
    "print(stemmed_words(corpus[0]))\n",
    "print(stemmed_words1(corpus[0]))\n",
    "\n",
    "#get feature names \n",
    "print(\"\\n\\nTFIDF Matrix after stemming and removing stop words\")\n",
    "print(cv1.get_feature_names_out())\n",
    "\n",
    "# DANGER! Convert to dense.\n",
    "m = cv1.transform(corpus).todense()\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa5d30f-fb8a-4f45-aa28-1d18baee2333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.42544054 0.         0.         0.54783215 0.\n",
      "  0.72033345]\n",
      " [0.52253528 0.30861775 0.         0.         0.7948031  0.\n",
      "  0.        ]\n",
      " [0.         0.32274454 0.54645401 0.54645401 0.         0.54645401\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#applying tf-idf term wieghting\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "#get counts \n",
    "counts = cv1.transform(corpus)\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "res = tfidf.todense()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035abe4-b666-4882-813a-0ab4eef3b113",
   "metadata": {},
   "source": [
    "#### Manually calculating tfidf for 'go' and 'tomorrow' \n",
    "tf-idf = tf * idf  \n",
    "idf(f) = log (1+n)/(1+df(t)) +1  \n",
    "vnorm = v / ||v||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df48ed29-e836-41ba-8faf-e8aa5810f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.4254405389711991\n",
      "True\n",
      "0.7203334490549893\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "documentSize = 3\n",
    "normvList = []\n",
    "\n",
    "#\"away\"\n",
    "documentsContainKey = 1\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 0\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"go\" \n",
    "documentsContainKey = 3\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 1\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "term_go =tfidfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"let\"\n",
    "documentsContainKey = 1\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 0\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"librari\" \n",
    "documentsContainKey = 1\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 0\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"rain\"\n",
    "documentsContainKey = 2\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 1\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"raini\" \n",
    "documentsContainKey = 1\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 0\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "#\"tomorrow\" \n",
    "documentsContainKey = 1\n",
    "idfTerm = math.log((documentSize+1)/(documentsContainKey+1)) +1\n",
    "#term frequency in this documnet\n",
    "tf = 1\n",
    "#multiply with idfTerm\n",
    "tfidfTerm = tf * idfTerm\n",
    "term_tom = tfidfTerm\n",
    "tfidfTerm = tfidfTerm**2 #square it \n",
    "normvList.append(tfidfTerm)\n",
    "\n",
    "normv = 0\n",
    "#add all elemnts in denominatory (normvlist)\n",
    "for v in normvList:\n",
    "    normv += v\n",
    "\n",
    "normv = math.sqrt(normv)\n",
    "\n",
    "#checking for term \"go\"\n",
    "print(res[0,1] == term_go/normv)\n",
    "print(term_go/normv)\n",
    "\n",
    "#checking for term \"tomorrow\"\n",
    "print(res[0,6] == term_tom/normv)\n",
    "print(term_tom/normv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed9305-b76c-4bc0-ba52-0bbc5b9dc745",
   "metadata": {},
   "source": [
    "**Estimate how large a corpus of 400,000 emails would be if represented in dense format.**\n",
    "I am going to assume that there will be 25,000 unique words for all 400,000 emails after trimming and stemming. Given that each element contains a floating number and stores 8 bytes of storage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c0e39d-4dbd-459a-baa8-e650232005ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes: 80000000000\n",
      "gigabytes: 80.0\n"
     ]
    }
   ],
   "source": [
    "#estimation = emails * uniqueWords * byteCount\n",
    "\n",
    "bytes= 400000*25000*8\n",
    "print(\"bytes:\" , bytes)\n",
    "gb = bytes/(10**9)\n",
    "print(\"gigabytes:\" , gb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
